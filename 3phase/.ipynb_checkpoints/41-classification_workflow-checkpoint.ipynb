{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Modeling-Walkthrough\" data-toc-modified-id=\"Modeling-Walkthrough-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Modeling Walkthrough</a></span><ul class=\"toc-item\"><li><span><a href=\"#Modeling-Steps\" data-toc-modified-id=\"Modeling-Steps-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Modeling Steps</a></span></li><li><span><a href=\"#The-Data\" data-toc-modified-id=\"The-Data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>The Data</a></span></li><li><span><a href=\"#Initial-Data-Understanding-and-Preparation\" data-toc-modified-id=\"Initial-Data-Understanding-and-Preparation-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Initial Data Understanding and Preparation</a></span></li></ul></li><li><span><a href=\"#1st-Model---&quot;Dummy&quot;-Model\" data-toc-modified-id=\"1st-Model---&quot;Dummy&quot;-Model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>1st Model - \"Dummy\" Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-Evaluation\" data-toc-modified-id=\"Model-Evaluation-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Model Evaluation</a></span></li></ul></li><li><span><a href=\"#2nd-Model---Logistic-Regression\" data-toc-modified-id=\"2nd-Model---Logistic-Regression-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>2nd Model - Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-Evaluation,-Part-2\" data-toc-modified-id=\"Model-Evaluation,-Part-2-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Model Evaluation, Part 2</a></span></li></ul></li><li><span><a href=\"#Back-to-Data-Preparation\" data-toc-modified-id=\"Back-to-Data-Preparation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Back to Data Preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Handling-Missing-Values\" data-toc-modified-id=\"Handling-Missing-Values-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Handling Missing Values</a></span></li><li><span><a href=\"#One-Hot-Encoding\" data-toc-modified-id=\"One-Hot-Encoding-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>One-Hot Encoding</a></span></li></ul></li><li><span><a href=\"#3rd-Model---After-More-Data-Preparation\" data-toc-modified-id=\"3rd-Model---After-More-Data-Preparation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>3rd Model - After More Data Preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameter-Adjustments-to-the-Model\" data-toc-modified-id=\"Hyperparameter-Adjustments-to-the-Model-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Hyperparameter Adjustments to the Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#More-Iterations\" data-toc-modified-id=\"More-Iterations-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>More Iterations</a></span></li><li><span><a href=\"#More-Regularization\" data-toc-modified-id=\"More-Regularization-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>More Regularization</a></span></li><li><span><a href=\"#Higher-Tolerance\" data-toc-modified-id=\"Higher-Tolerance-6.1.3\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;</span>Higher Tolerance</a></span></li></ul></li><li><span><a href=\"#Model-Evaluation,-Part-3\" data-toc-modified-id=\"Model-Evaluation,-Part-3-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Model Evaluation, Part 3</a></span></li></ul></li><li><span><a href=\"#Even-More-Data-Preparation---Scaling\" data-toc-modified-id=\"Even-More-Data-Preparation---Scaling-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Even More Data Preparation - Scaling</a></span></li><li><span><a href=\"#4th-Model---After-Scaling\" data-toc-modified-id=\"4th-Model---After-Scaling-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>4th Model - After Scaling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-Evaluation,-Part-4\" data-toc-modified-id=\"Model-Evaluation,-Part-4-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Model Evaluation, Part 4</a></span></li><li><span><a href=\"#Hyperparameter-Adjustment\" data-toc-modified-id=\"Hyperparameter-Adjustment-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Hyperparameter Adjustment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Different-Regularization-Strengths\" data-toc-modified-id=\"Different-Regularization-Strengths-8.2.1\"><span class=\"toc-item-num\">8.2.1&nbsp;&nbsp;</span>Different Regularization Strengths</a></span></li><li><span><a href=\"#Different-Solvers\" data-toc-modified-id=\"Different-Solvers-8.2.2\"><span class=\"toc-item-num\">8.2.2&nbsp;&nbsp;</span>Different Solvers</a></span></li></ul></li><li><span><a href=\"#SelectFromModel\" data-toc-modified-id=\"SelectFromModel-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span><code>SelectFromModel</code></a></span></li></ul></li><li><span><a href=\"#Final-Model-Evaluation\" data-toc-modified-id=\"Final-Model-Evaluation-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Final Model Evaluation</a></span></li><li><span><a href=\"#GridSearchCV\" data-toc-modified-id=\"GridSearchCV-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>GridSearchCV</a></span><ul class=\"toc-item\"><li><span><a href=\"#Choice-of-Grid-Values\" data-toc-modified-id=\"Choice-of-Grid-Values-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Choice of Grid Values</a></span></li></ul></li><li><span><a href=\"#Compare-the-past-models\" data-toc-modified-id=\"Compare-the-past-models-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Compare the past models</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:24.242016Z",
     "start_time": "2023-09-25T21:32:22.582594Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# plot_confusion_matrix is a handy visual tool, added in the latest version of scikit-learn\n",
    "# if you are running an older version, comment out this line and just use confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The goal here is to illustrate a possible workflow for classification modeling with `sklearn`'s `LogisticRegression` model.\n",
    "\n",
    "\n",
    "- Formulate and implement an iterative modeling workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caution! This notebook is very long and we will likely not get through it all. But the good news is that the modeling process is *iterative* and so after a few of those iterations you should get the hang of it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Build a model based on the [Titanic dataset](https://www.kaggle.com/c/titanic/data) that predicts whether a given person survived or not\n",
    "2. Evaluate the performance of the model\n",
    "3. Make changes in an attempt to improve the model\n",
    "4. Demonstrate whether an improvement was made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has the following columns:\n",
    "\n",
    "| Variable | Definition | Key |\n",
    "| -------- | ---------- | --- |\n",
    "| survival | Survival | 0 = No, 1 = Yes |\n",
    "| pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd |\n",
    "| sex | Sex | |\n",
    "| Age | Age in years | |\n",
    "| sibsp | # of siblings / spouses aboard the Titanic | |\n",
    "| parch | # of parents / children aboard the Titanic | |\n",
    "| ticket | Ticket number | |\n",
    "| fare | Passenger fare | |\n",
    "| cabin | Cabin number | |\n",
    "| embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Understanding and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the file, get everything into `X` features and `y` target variables, divided into train and test. Numerics only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:24.274018Z",
     "start_time": "2023-09-25T21:32:24.243003Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:24.304964Z",
     "start_time": "2023-09-25T21:32:24.275964Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:24.320963Z",
     "start_time": "2023-09-25T21:32:24.305963Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:24.365964Z",
     "start_time": "2023-09-25T21:32:24.321963Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:24.381964Z",
     "start_time": "2023-09-25T21:32:24.366963Z"
    }
   },
   "outputs": [],
   "source": [
    "numerical_columns = ['Survived','SibSp','Parch','Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.156965Z",
     "start_time": "2023-09-25T21:32:24.382964Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df[numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.171973Z",
     "start_time": "2023-09-25T21:32:27.158935Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_df = df[numerical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.187965Z",
     "start_time": "2023-09-25T21:32:27.173966Z"
    }
   },
   "outputs": [],
   "source": [
    "X = numeric_df.drop(columns=['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.203961Z",
     "start_time": "2023-09-25T21:32:27.188967Z"
    }
   },
   "outputs": [],
   "source": [
    "y = numeric_df.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.219962Z",
     "start_time": "2023-09-25T21:32:27.205937Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Model - \"Dummy\" Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a completely \"dummy\" model, that will always choose the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.235961Z",
     "start_time": "2023-09-25T21:32:27.221943Z"
    }
   },
   "outputs": [],
   "source": [
    "dummy_model = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.250976Z",
     "start_time": "2023-09-25T21:32:27.236951Z"
    }
   },
   "outputs": [],
   "source": [
    "dummy_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.266938Z",
     "start_time": "2023-09-25T21:32:27.251936Z"
    }
   },
   "outputs": [],
   "source": [
    "dummy_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.282936Z",
     "start_time": "2023-09-25T21:32:27.267935Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some cross-validation to see how the model would do in generalizing to new data it's never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.298962Z",
     "start_time": "2023-09-25T21:32:27.283937Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_results = cross_val_score(dummy_model, X_train, y_train, cv=5)\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the spread, let's make a convenient class that can help us organize the model and the cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.314977Z",
     "start_time": "2023-09-25T21:32:27.299970Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ModelWithCV():\n",
    "    '''Structure to save the model and more easily see its crossvalidation'''\n",
    "    \n",
    "    def __init__(self, model, model_name, X, y, cv_now=True):\n",
    "        self.model = model\n",
    "        self.name = model_name\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        # For CV results\n",
    "        self.cv_results = None\n",
    "        self.cv_mean = None\n",
    "        self.cv_median = None\n",
    "        self.cv_std = None\n",
    "        #\n",
    "        if cv_now:\n",
    "            self.cross_validate()\n",
    "        \n",
    "    def cross_validate(self, X=None, y=None, kfolds=10):\n",
    "        '''\n",
    "        Perform cross-validation and return results.\n",
    "        \n",
    "        Args: \n",
    "          X:\n",
    "            Optional; Training data to perform CV on. Otherwise use X from object\n",
    "          y:\n",
    "            Optional; Training data to perform CV on. Otherwise use y from object\n",
    "          kfolds:\n",
    "            Optional; Number of folds for CV (default is 10)  \n",
    "        '''\n",
    "        \n",
    "        cv_X = X if X else self.X\n",
    "        cv_y = y if y else self.y\n",
    "\n",
    "        self.cv_results = cross_val_score(self.model, cv_X, cv_y, cv=kfolds)\n",
    "        self.cv_mean = np.mean(self.cv_results)\n",
    "        self.cv_median = np.median(self.cv_results)\n",
    "        self.cv_std = np.std(self.cv_results)\n",
    "\n",
    "        \n",
    "    def print_cv_summary(self):\n",
    "        cv_summary = (\n",
    "        f'''CV Results for `{self.name}` model:\n",
    "            {self.cv_mean:.5f} Â± {self.cv_std:.5f} accuracy\n",
    "        ''')\n",
    "        print(cv_summary)\n",
    "\n",
    "        \n",
    "    def plot_cv(self, ax):\n",
    "        '''\n",
    "        Plot the cross-validation values using the array of results and given \n",
    "        Axis for plotting.\n",
    "        '''\n",
    "        ax.set_title(f'CV Results for `{self.name}` Model')\n",
    "        # Thinner violinplot with higher bw\n",
    "        sns.violinplot(y=self.cv_results, ax=ax, bw=.4)\n",
    "        sns.swarmplot(\n",
    "                y=self.cv_results,\n",
    "                color='orange',\n",
    "                size=10,\n",
    "                alpha= 0.8,\n",
    "                ax=ax\n",
    "        )\n",
    "\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do it for dummy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.346936Z",
     "start_time": "2023-09-25T21:32:27.315935Z"
    }
   },
   "outputs": [],
   "source": [
    "dummy_model_results = ModelWithCV(\n",
    "    model=dummy_model,\n",
    "    model_name = 'Dummy',\n",
    "    X=X_train,\n",
    "    y=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.500964Z",
     "start_time": "2023-09-25T21:32:27.347937Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax = dummy_model_results.plot_cv(ax)\n",
    "plt.tight_layout();\n",
    "\n",
    "dummy_model_results.print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.671946Z",
     "start_time": "2023-09-25T21:32:27.501971Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.suptitle(\"Dummy Model\")\n",
    "\n",
    "plot_confusion_matrix(dummy_model, X_train, y_train, ax=ax, cmap=\"plasma\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.702975Z",
     "start_time": "2023-09-25T21:32:27.676947Z"
    }
   },
   "outputs": [],
   "source": [
    "# just the numbers (this should work even with older scikit-learn)\n",
    "confusion_matrix(y_train, dummy_model.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pretty lopsided confusion matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.875944Z",
     "start_time": "2023-09-25T21:32:27.703976Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_roc_curve(dummy_model, X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a logistic regression and compare its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to specifically avoid any regularization (the default) to see how the model does with little change. So we'll pass `'none'` to the `penalty` parameter to not use any regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.905944Z",
     "start_time": "2023-09-25T21:32:27.879946Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_logreg_model = LogisticRegression(penalty='none', random_state=42).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:27.921945Z",
     "start_time": "2023-09-25T21:32:27.908945Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_logreg_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixture of 1s and 0s this time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation, Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModelWithCV for simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.077944Z",
     "start_time": "2023-09-25T21:32:27.922944Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_logreg_results = ModelWithCV(\n",
    "    model=simple_logreg_model,\n",
    "    model_name = 'Simple LogReg',\n",
    "    X=X_train,\n",
    "    y=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.217944Z",
     "start_time": "2023-09-25T21:32:28.078944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving variable for convenience\n",
    "model_results = simple_logreg_results\n",
    "\n",
    "# Plot CV results\n",
    "fig, ax = plt.subplots()\n",
    "ax = model_results.plot_cv(ax)\n",
    "plt.tight_layout();\n",
    "# Print CV results\n",
    "model_results.print_cv_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the mean accuracy is better when the model is actually taking in information from the features instead of always guessing the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.421946Z",
     "start_time": "2023-09-25T21:32:28.219946Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.suptitle(\"Logistic Regression with Numeric Features Only\")\n",
    "\n",
    "plot_confusion_matrix(simple_logreg_model, X_train, y_train, ax=ax, cmap=\"plasma\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.591944Z",
     "start_time": "2023-09-25T21:32:28.422946Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ROC\n",
    "plot_roc_curve(simple_logreg_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe there is some useful information in the features we are not using yet.  Let's go wild and add all of them!\n",
    "\n",
    "> Note: you can and should add features incrementally in a \"real\" modeling context.  The engineering effort of encoding the variables can be non-trivial!  But here let's assume that it's not too much work to encode all of them.\n",
    "\n",
    "Start with a new train-test split that contains all of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.607946Z",
     "start_time": "2023-09-25T21:32:28.592945Z"
    }
   },
   "outputs": [],
   "source": [
    "# TTS\n",
    "X=df.drop('Survived', axis=1)\n",
    "y=df.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.623944Z",
     "start_time": "2023-09-25T21:32:28.608947Z"
    }
   },
   "outputs": [],
   "source": [
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.639945Z",
     "start_time": "2023-09-25T21:32:28.624945Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's be extra cautious and make a separate column to indicate whether there originally was a missing value.\n",
    "\n",
    "In our training data there are only missing values for a couple of the columns, but we can't be sure about where the test set will be missing data.\n",
    "\n",
    "The [`MissingIndicator`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html) from `sklearn` will mark the missing values in an input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.655945Z",
     "start_time": "2023-09-25T21:32:28.640945Z"
    }
   },
   "outputs": [],
   "source": [
    "indicator_demo = MissingIndicator(features='all')\n",
    "\n",
    "indicator_demo.fit(X_train)\n",
    "\n",
    "indicator_demo.features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.671975Z",
     "start_time": "2023-09-25T21:32:28.656946Z"
    }
   },
   "outputs": [],
   "source": [
    "indicator_demo.transform(X_train)[:5, [3,8,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.687971Z",
     "start_time": "2023-09-25T21:32:28.672978Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.iloc[:5, [3, 8, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.702946Z",
     "start_time": "2023-09-25T21:32:28.688971Z"
    }
   },
   "outputs": [],
   "source": [
    "indicator = MissingIndicator(features=\"all\")\n",
    "indicator.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.717975Z",
     "start_time": "2023-09-25T21:32:28.704945Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_missing_indicator_columns(X, indicator):\n",
    "    \"\"\"\n",
    "    Helper function for transforming features\n",
    "    \n",
    "    For every feature in X, create another feature indicating whether that feature\n",
    "    is missing. (This doubles the number of columns in X.)\n",
    "    \"\"\"\n",
    "    \n",
    "    # create a 2D array of True and False values indicating whether a given feature\n",
    "    # is missing for that row\n",
    "    missing_array_bool = indicator.transform(X)\n",
    "    \n",
    "    # transform into 1 and 0 for modeling\n",
    "    missing_array_int = missing_array_bool.astype(int)\n",
    "    \n",
    "    # helpful for readability but not needed for modeling\n",
    "    missing_column_names = [col + \"_missing\" for col in X.columns]\n",
    "    \n",
    "    # convert to df so it we can concat with X\n",
    "    missing_df = pd.DataFrame(missing_array_int, columns=missing_column_names, index=X.index)\n",
    "    \n",
    "    return pd.concat([X, missing_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicator for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.733972Z",
     "start_time": "2023-09-25T21:32:28.718945Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = add_missing_indicator_columns(X_train, indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.764975Z",
     "start_time": "2023-09-25T21:32:28.734981Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've specified which values were originally missing, let's fill in those missing values.  This takes two separate imputers because we want to use the mean for numeric data and the majority class for categorical data.\n",
    "\n",
    "The `SimpleImputer` class fills in the mean value by default, so we'll have to override that for the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.779970Z",
     "start_time": "2023-09-25T21:32:28.765977Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_feature_names = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "categorical_feature_names = [\"Pclass\", \"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"]\n",
    "\n",
    "X_train_numeric = X_train[numeric_feature_names]\n",
    "X_train_categorical = X_train[categorical_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.795975Z",
     "start_time": "2023-09-25T21:32:28.780971Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_imputer = SimpleImputer(strategy='most_frequent').fit(X_train_categorical)\n",
    "numeric_imputer = SimpleImputer().fit(X_train_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a function here to minimize our work of imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.812013Z",
     "start_time": "2023-09-25T21:32:28.796980Z"
    }
   },
   "outputs": [],
   "source": [
    "def impute_missing_values(X, imputer):\n",
    "    \"\"\"\n",
    "    Given a DataFrame and an imputer, use the imputer to fill in all\n",
    "    missing values in the DataFrame\n",
    "    \"\"\"\n",
    "    imputed_array = imputer.transform(X)\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=X.columns, index=X.index)\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-check to make sure that all of the missing values are gone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.827013Z",
     "start_time": "2023-09-25T21:32:28.813014Z"
    }
   },
   "outputs": [],
   "source": [
    "# Numerics!\n",
    "X_train_numeric = impute_missing_values(X_train_numeric, numeric_imputer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.843014Z",
     "start_time": "2023-09-25T21:32:28.828012Z"
    }
   },
   "outputs": [],
   "source": [
    "# Categoricals!\n",
    "\n",
    "X_train_categorical = impute_missing_values(X_train_categorical, categorical_imputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.858982Z",
     "start_time": "2023-09-25T21:32:28.844984Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.874982Z",
     "start_time": "2023-09-25T21:32:28.860982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Double Check\n",
    "X_train_imputed = pd.concat([X_train_categorical,X_train_numeric], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.906019Z",
     "start_time": "2023-09-25T21:32:28.876014Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all of the old columns from X_train, then concat the new imputed ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.921982Z",
     "start_time": "2023-09-25T21:32:28.907985Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_imputed.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.936983Z",
     "start_time": "2023-09-25T21:32:28.923982Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.952983Z",
     "start_time": "2023-09-25T21:32:28.937982Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.967983Z",
     "start_time": "2023-09-25T21:32:28.953983Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numeric_feature_names + categorical_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:28.983982Z",
     "start_time": "2023-09-25T21:32:28.969982Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop(numeric_feature_names + categorical_feature_names, axis=1)\n",
    "X_train = pd.concat([X_train_imputed, X_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:29.015983Z",
     "start_time": "2023-09-25T21:32:28.985983Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:29.030983Z",
     "start_time": "2023-09-25T21:32:29.016983Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:29.046983Z",
     "start_time": "2023-09-25T21:32:29.031983Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that there are no missing values, convert all of the categorical features into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:29.062982Z",
     "start_time": "2023-09-25T21:32:29.047983Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_and_concat_feature_train(X_train, feature_name):\n",
    "    \"\"\"\n",
    "    Helper function for transforming training data.  It takes in the full X dataframe and\n",
    "    feature name, makes a one-hot encoder, and returns the encoder as well as the dataframe\n",
    "    with that feature transformed into multiple columns of 1s and 0s\n",
    "    \"\"\"\n",
    "    # make a one-hot encoder and fit it to the training data\n",
    "    ohe = OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\")\n",
    "    single_feature_df = X_train[[feature_name]]\n",
    "    ohe.fit(single_feature_df)\n",
    "    \n",
    "    # call helper function that actually encodes the feature and concats it\n",
    "    X_train = encode_and_concat_feature(X_train, feature_name, ohe)\n",
    "    \n",
    "    return ohe, X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:29.078982Z",
     "start_time": "2023-09-25T21:32:29.063982Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_and_concat_feature(X, feature_name, ohe):\n",
    "    \"\"\"\n",
    "    Helper function for transforming a feature into multiple columns of 1s and 0s. Used\n",
    "    in both training and testing steps.  Takes in the full X dataframe, feature name, \n",
    "    and encoder, and returns the dataframe with that feature transformed into multiple\n",
    "    columns of 1s and 0s\n",
    "    \"\"\"\n",
    "    # create new one-hot encoded df based on the feature\n",
    "    single_feature_df = X[[feature_name]]\n",
    "    feature_array = ohe.transform(single_feature_df).toarray()\n",
    "    ohe_df = pd.DataFrame(feature_array, columns=ohe.categories_[0], index=X.index)\n",
    "    \n",
    "    # drop the old feature from X and concat the new one-hot encoded df\n",
    "    X = X.drop(feature_name, axis=1)\n",
    "    X = pd.concat([X, ohe_df], axis=1)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:29.173982Z",
     "start_time": "2023-09-25T21:32:29.079983Z"
    }
   },
   "outputs": [],
   "source": [
    "encoders = {}\n",
    "\n",
    "for categorical_feature in categorical_feature_names:\n",
    "    ohe, X_train = encode_and_concat_feature_train(X_train, categorical_feature)\n",
    "    encoders[categorical_feature] = ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:29.189983Z",
     "start_time": "2023-09-25T21:32:29.174982Z"
    }
   },
   "outputs": [],
   "source": [
    "encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is...a ridiculous number of columns.  How did we end up with more columns than rows?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:29.220983Z",
     "start_time": "2023-09-25T21:32:29.191984Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:29.236983Z",
     "start_time": "2023-09-25T21:32:29.222982Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd Model - After More Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a logistic regression on our ridiculous number of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:29.362982Z",
     "start_time": "2023-09-25T21:32:29.237983Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# logreg\n",
    "\n",
    "log_reg_model = LogisticRegression(penalty='none',random_state=42).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened there?  This solver had no problem before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Adjustments to the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a couple of stopgap measures to get the model to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allows for more iterations to find a solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:29.378987Z",
     "start_time": "2023-09-25T21:32:29.363984Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg_model_more_iterations = LogisticRegression(penalty='none',\n",
    "                                                  random_state=42,\n",
    "                                                  max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:30.241983Z",
     "start_time": "2023-09-25T21:32:29.379982Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg_model_more_iterations.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the `C` parameter is the inverse of the regularization strength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: We could do regularization but we should first scale our features. We're actually going to skip this hyperparameter until we scale our data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher Tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [higher tolerance](https://medium.com/analytics-vidhya/a-complete-understanding-of-how-the-logistic-regression-can-perform-classification-a8e951d31c76) means that the model will stop training earlier (when predictions and true values aren't as close as they could be)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:30.257982Z",
     "start_time": "2023-09-25T21:32:30.242982Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logreg_model_higher_tolerance = LogisticRegression(penalty='none',\n",
    "                                                  random_state=42,\n",
    "                                                  tol=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:30.319983Z",
     "start_time": "2023-09-25T21:32:30.258982Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg_model_higher_tolerance.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation, Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:30.742096Z",
     "start_time": "2023-09-25T21:32:30.320983Z"
    }
   },
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n",
    "\n",
    "axes[0].set_title(\"More Iterations\")\n",
    "axes[1].set_title(\"Higher Tolerance\")\n",
    "\n",
    "plot_confusion_matrix(logreg_model_more_iterations, X_train, y_train,\n",
    "                      ax=axes[0], cmap=\"plasma\")\n",
    "plot_confusion_matrix(logreg_model_higher_tolerance, X_train, y_train,\n",
    "                      ax=axes[1], cmap=\"plasma\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:35.851097Z",
     "start_time": "2023-09-25T21:32:30.743096Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg_model_more_iterations_results = ModelWithCV(\n",
    "                                        logreg_model_more_iterations,\n",
    "                                        'more_iterations',\n",
    "                                        X_train,\n",
    "                                        y_train\n",
    ")\n",
    "    \n",
    "logreg_model_higher_tolerance_results = ModelWithCV(\n",
    "                                        logreg_model_higher_tolerance,\n",
    "                                        'higher_tolerance',\n",
    "                                        X_train,\n",
    "                                        y_train\n",
    ")\n",
    "\n",
    "model_results = [\n",
    "    logreg_model_more_iterations_results,\n",
    "    logreg_model_higher_tolerance_results\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:36.085122Z",
     "start_time": "2023-09-25T21:32:35.860127Z"
    }
   },
   "outputs": [],
   "source": [
    "f,axes = plt.subplots(ncols=2, sharey=True, figsize=(12, 6))\n",
    "\n",
    "for ax, result in zip(axes, model_results):\n",
    "    ax = result.plot_cv(ax)\n",
    "    result.print_cv_summary()\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:36.274091Z",
     "start_time": "2023-09-25T21:32:36.089097Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plot_roc_curve(logreg_model_more_iterations, X_train, y_train, \n",
    "               name='logreg_model_more_iterations', ax=ax)\n",
    "plot_roc_curve(logreg_model_higher_tolerance, X_train, y_train, \n",
    "               name='logreg_model_higher_tolerance', ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What can we observe from these two adjustments to our model with more features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Even More Data Preparation - Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that our last model is overfitting on so many features. A good strategy is to do regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, recall we should scale all of the features, so the model isn't overly penalizing age and fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:36.320121Z",
     "start_time": "2023-09-25T21:32:36.276091Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:36.336090Z",
     "start_time": "2023-09-25T21:32:36.321120Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale_values(X, scaler):\n",
    "    \"\"\"\n",
    "    Given a DataFrame and a fitted scaler, use the scaler to scale all of the features\n",
    "    \"\"\"\n",
    "    scaled_array = scaler.transform(X)\n",
    "    scaled_df = pd.DataFrame(scaled_array, columns=X.columns, index=X.index)\n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:36.368093Z",
     "start_time": "2023-09-25T21:32:36.338091Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = scale_values(X_train, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:36.400123Z",
     "start_time": "2023-09-25T21:32:36.369091Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4th Model - After Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is scaled, let's see if we can fit the model without tweaking any hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:36.448091Z",
     "start_time": "2023-09-25T21:32:36.401091Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(random_state=42)\n",
    "\n",
    "logreg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation, Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are able to run a logistic regression with default hyperparameters, let's see how that performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:36.651091Z",
     "start_time": "2023-09-25T21:32:36.449090Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.suptitle(\"Logistic Regression with All Features, Scaled\")\n",
    "\n",
    "plot_confusion_matrix(logreg_model, X_train, y_train, ax=ax, cmap=\"plasma\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:37.135858Z",
     "start_time": "2023-09-25T21:32:36.652091Z"
    }
   },
   "outputs": [],
   "source": [
    "all_features_results = ModelWithCV(\n",
    "                            logreg_model,\n",
    "                            'all_features',\n",
    "                            X_train,\n",
    "                            y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:37.259834Z",
     "start_time": "2023-09-25T21:32:37.136861Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving variable for convenience\n",
    "model_results = all_features_results\n",
    "\n",
    "# Plot CV results\n",
    "fig, ax = plt.subplots()\n",
    "ax = model_results.plot_cv(ax)\n",
    "plt.tight_layout();\n",
    "# Print CV results\n",
    "model_results.print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:37.398868Z",
     "start_time": "2023-09-25T21:32:37.261868Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_roc_curve(logreg_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect on the training data: this might be overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:37.414835Z",
     "start_time": "2023-09-25T21:32:37.399868Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(list(zip(X_train.columns, logreg_model.coef_[0])),\n",
    "       key=lambda x: abs(x[1]), reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Regularization Strengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out some different regularization penalties to see if we can improve the test data score a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:37.429870Z",
     "start_time": "2023-09-25T21:32:37.415836Z"
    }
   },
   "outputs": [],
   "source": [
    "all_features_results.print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:39.194864Z",
     "start_time": "2023-09-25T21:32:37.430835Z"
    }
   },
   "outputs": [],
   "source": [
    "model_results = [all_features_results]\n",
    "C_values = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "for c in C_values:\n",
    "    logreg_model = LogisticRegression(random_state=2021, C=c)\n",
    "    logreg_model.fit(X_train, y_train)\n",
    "    # Save Results\n",
    "    new_model_results = ModelWithCV(\n",
    "                            logreg_model,\n",
    "                            f'all_features_c{c:e}',\n",
    "                            X_train,\n",
    "                            y_train\n",
    "    )\n",
    "    model_results.append(new_model_results)\n",
    "    new_model_results.print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:39.880518Z",
     "start_time": "2023-09-25T21:32:39.195864Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f,axes = plt.subplots(ncols=3, nrows=2, sharey='all', figsize=(18, 12))\n",
    "\n",
    "for ax,result in zip(axes.ravel(),model_results):\n",
    "    ax = result.plot_cv(ax)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the default C value is pretty optimal for this solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:39.895502Z",
     "start_time": "2023-09-25T21:32:39.881472Z"
    }
   },
   "outputs": [],
   "source": [
    "model_results = [all_features_results]\n",
    "all_features_cross_val_score = all_features_results.cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try also some other solvers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:40.052472Z",
     "start_time": "2023-09-25T21:32:39.896472Z"
    }
   },
   "outputs": [],
   "source": [
    "# liblinear\n",
    "logreg_model = LogisticRegression(random_state=42, solver=\"liblinear\")\n",
    "logreg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:41.477472Z",
     "start_time": "2023-09-25T21:32:40.053472Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save for later comparison\n",
    "model_results.append(\n",
    "    ModelWithCV(\n",
    "        logreg_model, \n",
    "        'solver:liblinear',\n",
    "        X_train,\n",
    "        y_train\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot both all_features vs new model\n",
    "f,axes = plt.subplots(ncols=2, sharey='all', figsize=(12, 6))\n",
    "\n",
    "model_results[0].plot_cv(ax=axes[0])\n",
    "model_results[-1].plot_cv(ax=axes[1])\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:41.493472Z",
     "start_time": "2023-09-25T21:32:41.478472Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Old:\", all_features_cross_val_score)\n",
    "print(\"New:\", model_results[-1].cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little lower, but no major difference in the scores.  Let's try adding some more regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:41.588505Z",
     "start_time": "2023-09-25T21:32:41.494472Z"
    }
   },
   "outputs": [],
   "source": [
    "# now with added regularization!\n",
    "logreg_model = LogisticRegression(random_state=42, solver=\"liblinear\", C=0.01)\n",
    "logreg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:42.611502Z",
     "start_time": "2023-09-25T21:32:41.590472Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save for later comparison\n",
    "model_results.append(\n",
    "    ModelWithCV(\n",
    "        logreg_model, \n",
    "        'solver:liblinear_C:0.01',\n",
    "        X_train,\n",
    "        y_train\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot both all_features vs new model\n",
    "f,axes = plt.subplots(ncols=2, sharey='all', figsize=(12, 6))\n",
    "\n",
    "model_results[0].plot_cv(ax=axes[0])\n",
    "model_results[-1].plot_cv(ax=axes[1])\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:42.627502Z",
     "start_time": "2023-09-25T21:32:42.612502Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Old:\", all_features_cross_val_score)\n",
    "print(\"New:\", model_results[-1].cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting better.  Try a different type of penalty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:42.722471Z",
     "start_time": "2023-09-25T21:32:42.628474Z"
    }
   },
   "outputs": [],
   "source": [
    "# That wasn't a penalty!\n",
    "logreg_model = LogisticRegression(random_state=42, solver=\"liblinear\", penalty=\"l1\")\n",
    "logreg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:43.805401Z",
     "start_time": "2023-09-25T21:32:42.723474Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save for later comparison\n",
    "model_results.append(\n",
    "    ModelWithCV(\n",
    "        logreg_model, \n",
    "        'solver:liblinear_penalty:l1',\n",
    "        X_train,\n",
    "        y_train\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot both all_features vs new model\n",
    "f,axes = plt.subplots(ncols=2, sharey='all', figsize=(12, 6))\n",
    "\n",
    "model_results[0].plot_cv(ax=axes[0])\n",
    "model_results[-1].plot_cv(ax=axes[1])\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:43.821396Z",
     "start_time": "2023-09-25T21:32:43.806372Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Old:\", all_features_cross_val_score)\n",
    "print(\"New:\", model_results[-1].cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly better average here.  Try adding some more regularization with L1 penalty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:43.884400Z",
     "start_time": "2023-09-25T21:32:43.823373Z"
    }
   },
   "outputs": [],
   "source": [
    "# Regular penalty?\n",
    "logreg_model = LogisticRegression(random_state=2021, solver=\"liblinear\", penalty=\"l1\", C=0.01)\n",
    "logreg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:44.617371Z",
     "start_time": "2023-09-25T21:32:43.885374Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save for later comparison\n",
    "model_results.append(\n",
    "    ModelWithCV(\n",
    "        logreg_model, \n",
    "        'solver:liblinear_penalty:l1_C:0.01',\n",
    "        X_train,\n",
    "        y_train\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot both all_features vs new model\n",
    "f,axes = plt.subplots(ncols=2, sharey='all', figsize=(12, 6))\n",
    "\n",
    "model_results[0].plot_cv(ax=axes[0])\n",
    "model_results[-1].plot_cv(ax=axes[1])\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:44.632370Z",
     "start_time": "2023-09-25T21:32:44.618370Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Old:\", all_features_cross_val_score)\n",
    "print(\"New:\", model_results[-1].cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, the default regularization strength seems pretty good.  Double-check the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:44.883406Z",
     "start_time": "2023-09-25T21:32:44.634371Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(random_state=42, solver=\"liblinear\", penalty=\"l1\")\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.suptitle(\"Logistic Regression with All Features (Scaled, Hyperparameters Tuned)\")\n",
    "\n",
    "plot_confusion_matrix(logreg_model, X_train, y_train, ax=ax, cmap=\"plasma\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SelectFromModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last model is probably overfitting. We might try thinning out the number of features by eliminating the ones with small modeling coefficients using [`SelectFromModel`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:44.899371Z",
     "start_time": "2023-09-25T21:32:44.884370Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:44.914372Z",
     "start_time": "2023-09-25T21:32:44.900401Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate and fit\n",
    "\n",
    "selector = SelectFromModel(logreg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.007370Z",
     "start_time": "2023-09-25T21:32:44.915372Z"
    }
   },
   "outputs": [],
   "source": [
    "selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using the default threshold here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.022370Z",
     "start_time": "2023-09-25T21:32:45.008370Z"
    }
   },
   "outputs": [],
   "source": [
    "thresh = selector.threshold_\n",
    "thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.038372Z",
     "start_time": "2023-09-25T21:32:45.024373Z"
    }
   },
   "outputs": [],
   "source": [
    "coefs = selector.estimator_.coef_\n",
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.053402Z",
     "start_time": "2023-09-25T21:32:45.039371Z"
    }
   },
   "outputs": [],
   "source": [
    "coefs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.069370Z",
     "start_time": "2023-09-25T21:32:45.055402Z"
    }
   },
   "outputs": [],
   "source": [
    "coefs[coefs > thresh].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sense of which features will be eliminated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.085399Z",
     "start_time": "2023-09-25T21:32:45.070402Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_important_features(X, selector):\n",
    "    \"\"\"\n",
    "    Given a DataFrame and a selector, use the selector to choose\n",
    "    the most important columns\n",
    "    \"\"\"\n",
    "    imps = dict(zip(X.columns, selector.get_support()))\n",
    "    selected_array = selector.transform(X)\n",
    "    selected_df = pd.DataFrame(selected_array,\n",
    "                               columns=[col for col in X.columns if imps[col]],\n",
    "                               index=X.index)\n",
    "    return selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.101371Z",
     "start_time": "2023-09-25T21:32:45.086372Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do it to Train\n",
    "X_train_selected = select_important_features(X_train, selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.131402Z",
     "start_time": "2023-09-25T21:32:45.103371Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.147402Z",
     "start_time": "2023-09-25T21:32:45.132372Z"
    }
   },
   "outputs": [],
   "source": [
    "# Logreg\n",
    "\n",
    "logreg_sel = LogisticRegression(random_state=42, solver='liblinear', penalty='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.694401Z",
     "start_time": "2023-09-25T21:32:45.149374Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save for later comparison\n",
    "select_results = ModelWithCV(\n",
    "                    logreg_sel, \n",
    "                    'logreg_sel',\n",
    "                    X_train_selected,\n",
    "                    y_train\n",
    ")\n",
    "\n",
    "# Plot both all_features vs new model\n",
    "f,axes = plt.subplots(ncols=2, sharey='all', figsize=(12, 6))\n",
    "\n",
    "model_results[0].plot_cv(ax=axes[0])\n",
    "select_results.plot_cv(ax=axes[1])\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.709401Z",
     "start_time": "2023-09-25T21:32:45.695397Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Old:\", all_features_cross_val_score)\n",
    "print(\"New:\", select_results.cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably still overfitting, but let's call this our final model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a final model, run X_test through all of the preprocessing steps so we can evaluate the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.725371Z",
     "start_time": "2023-09-25T21:32:45.710371Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_no_transformations = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.741402Z",
     "start_time": "2023-09-25T21:32:45.727372Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.757370Z",
     "start_time": "2023-09-25T21:32:45.742372Z"
    }
   },
   "outputs": [],
   "source": [
    "# add missing indicators\n",
    "\n",
    "X_test_mi = add_missing_indicator_columns(X_test_no_transformations, indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.773371Z",
     "start_time": "2023-09-25T21:32:45.759372Z"
    }
   },
   "outputs": [],
   "source": [
    "# separate out values for imputation\n",
    "X_test_numeric = X_test_mi[numeric_feature_names]\n",
    "X_test_categorical = X_test_mi[categorical_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.788371Z",
     "start_time": "2023-09-25T21:32:45.774371Z"
    }
   },
   "outputs": [],
   "source": [
    "# impute missing values\n",
    "X_test_numeric = impute_missing_values(X_test_numeric, numeric_imputer)\n",
    "X_test_categorical = impute_missing_values(X_test_categorical, categorical_imputer)\n",
    "X_test_imputed = pd.concat([X_test_numeric, X_test_categorical], axis=1)\n",
    "X_test_new = X_test_mi.drop(numeric_feature_names + categorical_feature_names, axis=1)\n",
    "X_test_final = pd.concat([X_test_imputed, X_test_new], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.834399Z",
     "start_time": "2023-09-25T21:32:45.789401Z"
    }
   },
   "outputs": [],
   "source": [
    "# one-hot encode categorical data\n",
    "for categorical_feature in categorical_feature_names:\n",
    "    X_test_final = encode_and_concat_feature(X_test_final,\n",
    "                                       categorical_feature, encoders[categorical_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.865400Z",
     "start_time": "2023-09-25T21:32:45.835371Z"
    }
   },
   "outputs": [],
   "source": [
    "# scale values\n",
    "X_test_scaled = scale_values(X_test_final, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.881372Z",
     "start_time": "2023-09-25T21:32:45.866370Z"
    }
   },
   "outputs": [],
   "source": [
    "# select features\n",
    "X_test_selected = select_important_features(X_test_scaled, selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model with the relevant hyperparameters, fit, and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.944403Z",
     "start_time": "2023-09-25T21:32:45.882373Z"
    }
   },
   "outputs": [],
   "source": [
    "final_model = LogisticRegression(random_state=2021, solver=\"liblinear\", penalty=\"l1\")\n",
    "final_model.fit(X_train_selected, y_train)\n",
    "\n",
    "final_model.score(X_test_selected, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Grid Searching\" allows to search over a number of different hyperparameters. [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) is the library we will be using.\n",
    "\n",
    "Note that [`RandomSeachCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) will be a better tool for a lot of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.960370Z",
     "start_time": "2023-09-25T21:32:45.945370Z"
    }
   },
   "outputs": [],
   "source": [
    "LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.975370Z",
     "start_time": "2023-09-25T21:32:45.961370Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build a grid\n",
    "grid = {\n",
    "    'penalty' : ['none','l1','l2'],\n",
    "    'max_iter' : [10,100,1000],\n",
    "    'C' : [0.000001, 0.00001, 0.0001, 0.001 ],\n",
    "    'tol' : [.0001, .001, .01, .1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:45.990370Z",
     "start_time": "2023-09-25T21:32:45.977371Z"
    }
   },
   "outputs": [],
   "source": [
    "3 * 3 *5 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:46.006370Z",
     "start_time": "2023-09-25T21:32:45.991370Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the object\n",
    "\n",
    "gs = GridSearchCV(LogisticRegression(), param_grid=grid, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:54.257371Z",
     "start_time": "2023-09-25T21:32:46.007371Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit\n",
    "\n",
    "gs.fit(X_train_selected, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:54.273371Z",
     "start_time": "2023-09-25T21:32:54.258372Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:54.288371Z",
     "start_time": "2023-09-25T21:32:54.275373Z"
    }
   },
   "outputs": [],
   "source": [
    "# Score\n",
    "gs.score(X_train_selected, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:54.303372Z",
     "start_time": "2023-09-25T21:32:54.290372Z"
    }
   },
   "outputs": [],
   "source": [
    "gs.score(X_test_selected, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:54.319370Z",
     "start_time": "2023-09-25T21:32:54.305377Z"
    }
   },
   "outputs": [],
   "source": [
    "# Estimator\n",
    "\n",
    "gs_model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:54.601371Z",
     "start_time": "2023-09-25T21:32:54.320371Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "gs_model_results = ModelWithCV(\n",
    "                        model=gs_model,\n",
    "                        model_name='gs_model',\n",
    "                        X=X_train_selected, \n",
    "                        y=y_train\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax = gs_model_results.plot_cv(ax)\n",
    "plt.tight_layout();\n",
    "\n",
    "gs_model_results.print_cv_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of Grid Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which values should you pick for your grid? Intuitively, you should try both \"large\" and \"small\" values, but of course what counts as large and small will really depend on the type of hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For a k-nearest neighbors model, 1 or 3 would be a small value for the number of neighbors and 15 or 17 would be a large value.\n",
    "- For a decision tree model, what counts as a small `max_depth` will really depend on the size of your training data. A `max_depth` of 5 would likely have little effect on a very small dataset but, at the same time, it would probably significantly decrease the variance of a model where the dataset is large.\n",
    "- For a logistic regression's regularization constant, you may want to try a set of values that are exponentially separated, like \\[1, 10, 100, 1000\\].\n",
    "- **If a grid search finds optimal values at the ends of your hyperparameter ranges, you might try another grid search with more extreme values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the past models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:54.617401Z",
     "start_time": "2023-09-25T21:32:54.603371Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create a way to categorize our different models\n",
    "model_candidates = [\n",
    "    {\n",
    "        'name':'dummy_model'\n",
    "        ,'model':dummy_model\n",
    "        ,'X_test':X_test\n",
    "        ,'y_test':y_test\n",
    "    },\n",
    "    {\n",
    "        'name':'simple_logreg_model'\n",
    "        ,'model':simple_logreg_model\n",
    "        ,'X_test':X_test_no_transformations[[\"SibSp\", \"Parch\", \"Fare\"]]\n",
    "        ,'y_test':y_test\n",
    "    },\n",
    "    {\n",
    "        'name':'logreg_model_more_iterations'\n",
    "        ,'model':logreg_model_more_iterations\n",
    "        ,'X_test':X_test_final\n",
    "        ,'y_test':y_test\n",
    "    },\n",
    "    {\n",
    "        'name':'logreg_model_higher_tolerance'\n",
    "        ,'model':logreg_model_higher_tolerance\n",
    "        ,'X_test':X_test_final\n",
    "        ,'y_test':y_test\n",
    "    },\n",
    "    {\n",
    "        'name':'final_model'\n",
    "        ,'model':final_model\n",
    "        ,'X_test':X_test_selected\n",
    "        ,'y_test':y_test\n",
    "    },\n",
    "    {\n",
    "        'name':'gs_model'\n",
    "        ,'model':gs_model\n",
    "        ,'X_test':X_test_selected\n",
    "        ,'y_test':y_test\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:54.665401Z",
     "start_time": "2023-09-25T21:32:54.619372Z"
    }
   },
   "outputs": [],
   "source": [
    "final_scores_dict = {\n",
    "    \"Model Name\": [candidate.get('name') for candidate in model_candidates],\n",
    "    \"Mean Accuracy\": [\n",
    "        candidate.get('model').score(\n",
    "                                candidate.get('X_test'), \n",
    "                                candidate.get('y_test')\n",
    "        ) \n",
    "        for candidate in model_candidates\n",
    "    ]\n",
    "    \n",
    "}\n",
    "final_scores_df = pd.DataFrame(final_scores_dict).set_index('Model Name')\n",
    "final_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final comparison of confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:55.585438Z",
     "start_time": "2023-09-25T21:32:54.666406Z"
    }
   },
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = math.ceil(len(model_candidates)/nrows)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "                nrows=nrows,\n",
    "                ncols=ncols,\n",
    "                figsize=(12, 6)\n",
    ")\n",
    "fig.suptitle(\"Confusion Matrix Comparison\")\n",
    "\n",
    "# Turn off all the axes (in case nothing to plot); turn on while iterating over\n",
    "[ax.axis('off') for ax in axes.ravel()]\n",
    "\n",
    "\n",
    "for i,candidate in enumerate(model_candidates):\n",
    "    # Logic for making rows and columns for matrices\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    ax = axes[row][col]\n",
    "    \n",
    "    ax.set_title(candidate.get('name'))\n",
    "    ax.set_axis_on() \n",
    "    cm_display = plot_confusion_matrix(\n",
    "                    candidate.get('model'),\n",
    "                    candidate.get('X_test'),\n",
    "                    candidate.get('y_test'),\n",
    "                    normalize='true',\n",
    "                    cmap='plasma',\n",
    "                    ax=ax,\n",
    "                    \n",
    "    )\n",
    "    cm_display.im_.set_clim(0, 1)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:55.774875Z",
     "start_time": "2023-09-25T21:32:55.586409Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot only the last models we created (so it's not too cluttered)\n",
    "for model_candidate in model_candidates[3:]:\n",
    "    plot_roc_curve(\n",
    "        model_candidate.get('model'),\n",
    "        model_candidate.get('X_test'),\n",
    "        model_candidate.get('y_test'), \n",
    "        name=model_candidate.get('name'),\n",
    "        ax=ax\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T21:32:56.057835Z",
     "start_time": "2023-09-25T21:32:55.775903Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "\n",
    "for model_candidate in model_candidates:\n",
    "    plot_roc_curve(\n",
    "        model_candidate.get('model'),\n",
    "        model_candidate.get('X_test'),\n",
    "        model_candidate.get('y_test'), \n",
    "        name=model_candidate.get('name'),\n",
    "        ax=ax\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
