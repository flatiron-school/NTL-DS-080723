{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability and Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from src.piApprox import pi_approx\n",
    "import src.monty_hall as mh\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: some of this text comes from [_OpenIntro Statistics_](https://www.openintro.org/book/os/), Chapter 3.\n",
    "\n",
    "In general, calculating probabilities is a matter of dividing the outcome you're exploring by all possible outcomes:\n",
    "\n",
    "$$\\large P(Event) = \\frac{|Event|}{|Sample\\ Space|} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are often interested in calculating the probability that *either* event $A$ or event $B$ occur, or the probability that *both* events $A$ and $B$ occur.\n",
    "\n",
    "In describing these sorts of probabilities, we generally borrow some notation from set theory and use the set \"union\" symbol ($\\cup$) for \"or\" and the set \"intersection\" symbol ($\\cap$) for \"and\".\n",
    "\n",
    "(If I collect the $A$ and $B$ possibilities into a set, then to ask about the probability of $A$ **or** $B$ is to ask about the probability of an event occurring from the **union** of $A$ and $B$, $A\\cup B$. Similarly, to ask about the probability of $A$ **and** $B$ is to ask about the probability of an event occuring from the **intersection** of $A$ and $B$, $A\\cap B$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Addition Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that either $A$ or $B$ will occur can be calculated by adding each individual probability, and then subtracting the probability that both occur together:\n",
    "\n",
    "$$\\large P(A \\cup B) = P(A) + P(B) âˆ’ P(A \\cap B)$$\n",
    "\n",
    "Remember, $P(A \\cap B)$ expresses the overlap between the two events - if you don't subtract that overlap, then you double count the instances when **both** $A$ and $B$ occur!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplication Rule for Independent Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A special condition is when the outcome of $A$ has no bearing on the outcome of $B$. We say these two events are **independent** (e.g. rolling a die and tossing a coin).\n",
    "\n",
    "If $A$ and $B$ represent events from two different and **independent** processes, then the probability\n",
    "that both $A$ and $B$ occur can be calculated as the product of their separate probabilities:\n",
    "\n",
    "$$\\large P(A \\cap B) = P(A) * P(B)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§  Knowledge Check\n",
    "\n",
    "### 1) AND Question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the probability of rolling a 5 on a fair die _and_ getting a tails on a fair coin toss?\n",
    "\n",
    "**Your answer here**:\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "We're checking for the intersection of these sets. Of the six possible outcomes on a die roll only one (the 5) will do. So the chance of getting a 5 on a die is 1/6. Of the two possible outcomes on a coin toss again only one (tails) will do. So the chance of getting tail on a coin toss is 1/2.\n",
    "\n",
    "So the calculation is: $$\\large P(5 \\cap tails) = \\left(\\frac{1}{6}\\right)*\\left(\\frac{1}{2}\\right) = \\frac{1}{12}$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) OR Question: \n",
    "\n",
    "What is the probability of rolling a 5 on a die _or_ getting a tails on a coin toss?\n",
    "\n",
    "**Your answer here**:\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "    \n",
    "   We're now checking for the union of these sets. Here we want to count all the die-coin combinations where we have a 5 on the die AND all the die-coin combinations where we have a tails on the coin. \n",
    "\n",
    "$$\\large P(5 \\cup tails) $$\n",
    "\n",
    "**BUT:**\n",
    "\n",
    "If the die is 5, that includes two possibilities: 5-heads and **5-tails**.\n",
    "\n",
    "Our coin is tails, that includes six possibilities: 1-tails, 2-tails, 3-tails, 4-tails, **5-tails**, and 6-tails.\n",
    "\n",
    "But then we've counted the combination where **both** the 5 and the tails occur **twice**.\n",
    "\n",
    "So the correct calculation is the sum of the individual probabilities **less the probability of their intersection**:\n",
    "\n",
    "$$\\large P(5 \\cup tails) = \\left(\\frac{1}{6}\\right) + \\left(\\frac{1}{2}\\right) - \\left(\\left(\\frac{1}{6}\\right)*\\left(\\frac{1}{2}\\right)\\right) = \\frac{7}{12} $$\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enough Talk - Let's Explore in Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mushroom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a modified version of the Mushroom dataset from UCI [here](https://archive.ics.uci.edu/ml/datasets/Mushroom). Each row in this dataset corresponds to one observation (one mushroom). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Mushrooms_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) If you picked a row from this dataset at random, what is the probability it corresponds to a bruised mushroom? \n",
    "\n",
    "In other words, find $P(bruised)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df.index))\n",
    "print(len(df.loc[df['bruised'] == True].index))\n",
    "\n",
    "len(df.loc[df['bruised'] == True]) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way\n",
    "p_bruised = df[df['bruised'] == True].shape[0]/df.shape[0]\n",
    "p_bruised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see...\n",
    "df.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) What is the probability you pick a row corresponding to a mushroom that is bruised _AND_ edible?\n",
    "\n",
    "$P(edible \\cap bruised)$\n",
    "\n",
    "BUT! Are they independent events ...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bruised_and_edible = df[(df['bruised'] == True) & (df['edible-poisonous'] == 'edible')].shape[0]/df.shape[0]\n",
    "\n",
    "p_bruised_and_edible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are being bruised and being edible independent of each other?\n",
    "\n",
    "> Formally, $A$ and $B$ are *independent* if and only if the probability that *both* $A$ *and* $B$ happen is:\n",
    "> \n",
    "> $$\\large P(A \\cap B) = P(A) * P(B)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bruised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_edible = len(df[df['edible-poisonous'] == 'edible'])/len(df)\n",
    "p_edible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bruised * p_edible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bruised_and_edible == p_bruised * p_edible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter: Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When do we compute conditional probabilities? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compute conditional probabilities when the outcome of an event depends on the outcome of previous events (**dependent** events). A conditional probability of an event is the probability of the event *given* another event has occurred.\n",
    "\n",
    "\n",
    "When events _are_ independent, the rule for probabilistic AND (I'll use '$\\cap$' below) is simple:\n",
    "\n",
    "$$\\large P(A\\cap B) = P(A) * P(B)$$\n",
    "\n",
    "But the more general rule, which includes non-independent events, is:\n",
    "\n",
    "$$\\large P(A\\cap B) = P(A | B) * P(B)$$\n",
    "\n",
    "> (this is pretty much the law of total probability, we'll revisit this later!)\n",
    "\n",
    "In fact, this is the definition of conditional probability. Rearranging:\n",
    "\n",
    "$$\\large P(A | B) = \\frac{P(A\\cap B)}{P(B)}$$\n",
    "\n",
    "The `|` here should be read as \"given\". We are given some information, $B$, and thus it reduces our sample space!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to our Mushrooms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) What is the probability of picking an edible mushroom given it is bruised? \n",
    "\n",
    "$P(edible | bruised)$\n",
    "\n",
    "p_edible_given_bruised = p_bruised_and_edible/p_bruised\n",
    "p_edible_given_bruised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bruised = df[df['bruised'] == True]\n",
    "bruised['edible-poisonous'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) What is the probability of picking a bruised mushroom given it is edible? \n",
    "\n",
    "$P(bruised | edible)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_edible = df[df['edible-poisonous'] == 'edible'].shape[0]/df.shape[0]\n",
    "p_edible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bruised_given_edible = p_bruised_and_edible/p_edible\n",
    "p_bruised_given_edible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edible = df[df['edible-poisonous'] == 'edible']\n",
    "edible['bruised'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition behind conditional probability: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you compute the probability that mushrooms are edible given they are bruised? \n",
    "\n",
    "When you ask the question \"what is the probability that the mushrooms are edible and bruised?\", the sample space originally contains all 8124 rows of mushrooms. \n",
    "\n",
    "<img src=\"images/Image_72_Cond4.png\" width=\"300\">\n",
    "\n",
    "However, to compute the probability that the mushrooms are edible given they are bruised, you need to consider the reduced size of the sample space. \n",
    "\n",
    "In the image above, S is the universe of all mushrooms in the dataset, A is the set of mushrooms that are edible, and B is the set of mushrooms that are bruised.\n",
    "\n",
    "* When you ask the question \"what is the probability that the mushrooms are edible given the mushrooms are bruised?\", you have effectively reduced the size of the sample space to include only those mushrooms that are bruised. \n",
    "\n",
    "* Given that mushrooms are bruised, the only way for the mushrooms to be edible is for these mushrooms to fall in the intersection of the set of mushrooms that are edible _and_ the set of mushrooms that are bruised , $P(edible \\cap bruised)$.  \n",
    "\n",
    "* To account for the smaller sample space, you divide the probability mushrooms are edible and bruised by the probability the mushrooms are bruised: $$\\large P(edible|bruised) = \\frac{P(edible \\cap bruised)}{P(bruised)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Law of Total Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want to calculate an **unconditional** probability by making use of some **conditional** probabilities. Enter the Law of Total Probability:\n",
    "\n",
    "$$\\large P(A)=\\sum _{n}P(A\\cap B_{n})$$\n",
    "\n",
    "Which works out to: \n",
    "\n",
    "$$\\large P(A)=\\sum _{n}P(A\\mid B_{n}) * P(B_{n})$$\n",
    "\n",
    "Where $B_{n}$ captures all of the disjoint events that combine to define the sample space, so $\\sum P(B_{n}) = 1$.\n",
    "\n",
    "#### In practice:\n",
    "\n",
    "Let's say we want to know the likelihood that a mushroom is **edible**. If we know the likelihood that all tapering stalk mushrooms are edible, and the likelihood that not-tapering-stalk mushrooms are edible, plus how likely it is that mushrooms have tapering stalks (and thus also how likely the mushroom does _not_ have a tapering stalk, the compliment of the mushrooms having a tapering stalk) - well, we can use all of that to find the likelihood our mushroom will be edible! \n",
    "\n",
    "$$\\large P(\\text{edible}) = P(\\text{edible} | \\text{tapering}) * P(\\text{tapering}) + P(\\text{edible} | \\text{not tapering}) * P(\\text{not tapering})$$\n",
    "\n",
    "We can do this because 'tapering stalk' and 'not tapering stalk' capture all of the possibilities of mushrooms - they define our entire sample space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's find our tapering and not-tapering subsets\n",
    "tapering = df.loc[df['stalk-shape'] == 'tapering']\n",
    "\n",
    "not_tapering = df.loc[df['stalk-shape'] != 'tapering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate our probability of tapering, and probability of not-tapering\n",
    "p_tapering = len(tapering) / len(df)\n",
    "\n",
    "p_not_tapering = len(not_tapering) / len(df)\n",
    "# same as \n",
    "# p_not_tapering = 1 - p_tapering\n",
    "\n",
    "# Now calculate our two conditional probabilties\n",
    "p_edible_given_tapering = len(\n",
    "    tapering.loc[tapering['edible-poisonous'] == 'edible']) / len(tapering)\n",
    "\n",
    "p_edible_given_not_tapering = len(\n",
    "    not_tapering.loc[not_tapering['edible-poisonous'] == 'edible']) / len(not_tapering)\n",
    "\n",
    "# Do the math!\n",
    "p_edible_given_tapering * p_tapering + p_edible_given_not_tapering * p_not_tapering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "len(df.loc[df['edible-poisonous'] == 'edible'])/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning Complex Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're not really a mushroom expert, but you can see that the mushroom in your hand is has some orange on the stalk. Given the data at your disposal, what's the probability that the mushroom is edible?\n",
    "\n",
    "\n",
    "\n",
    "$$\\large P(\\text{edible|orange stalk}) = \\frac{P(\\text{edible} \\cap \\text{orange stalk})}{P(\\text{orange stalk})}$$\n",
    "\n",
    "Furthermore, we can decompose $P(\\text{orange stalk})$ into the two possibilities:\n",
    "\n",
    "$P(\\text{orange stalk}) = P(\\text{orange stalk below ring}\\cup\\text{orange stalk above ring})$\n",
    "\n",
    "But be careful here! \n",
    "\n",
    "$P(\\text{orange stalk below ring}\\cup \\text{orange stalk above ring})$ **DOES NOT EQUAL** $P(\\text{orange stalk below ring}) + P(\\text{orange stalk above ring})$\n",
    "\n",
    "While this may seem correct, adding these individual probabilities **double counts** mushrooms which have entirely orange stalks. However - when we do these things in Pandas (using a loc and an or condition), it won't duplicate the row - so you don't need to worry about it here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can prove that P(orange above) + P(orange below) != p(orange stalk)\n",
    "p_orange_above = df[df['stalk-color-above-ring'] == 'orange'].shape[0]/df.shape[0]\n",
    "p_orange_below = df[df['stalk-color-below-ring'] == 'orange'].shape[0]/df.shape[0]\n",
    "\n",
    "p_orange_stalk = df[(df['stalk-color-above-ring'] == 'orange')\n",
    "                    | (df['stalk-color-below-ring'] == 'orange')\n",
    "                    ].shape[0]/df.shape[0]\n",
    "\n",
    "p_orange_stalk == (p_orange_above + p_orange_below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's find P(edible | orange stalk)\n",
    "p_edible_and_orange = df[((df['stalk-color-above-ring'] == 'orange')\n",
    "                          | (df['stalk-color-below-ring'] == 'orange')\n",
    "                          )\n",
    "                         & (df['edible-poisonous'] == 'edible')\n",
    "                         ].shape[0]/df.shape[0]\n",
    "\n",
    "p_edible_given_orange = p_edible_and_orange / p_orange_stalk\n",
    "\n",
    "p_edible_given_orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apparently orange-stalk mushrooms seem fairly safe....\n",
    "# (Disclaimer: don't take this as definitive foraging adivce!)\n",
    "\n",
    "# Sanity check...\n",
    "df.loc[(df['stalk-color-above-ring'] == 'orange')\n",
    "       | (df['stalk-color-below-ring'] == 'orange')]['edible-poisonous'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit: Additional Conditional Probability Practice\n",
    "\n",
    "What's the probability that a mushroom is poisonous if it has close gill spacing AND a tapering stalk?\n",
    "\n",
    "$$\\large P(poisonous|close \\cap tapering) = \\frac{P(poisonous \\cap close \\cap tapering)}{P(close \\cap tapering)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P that mushroom is poisonous given close gill spacing and tapering stalk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesianism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian treats probabilities as **subjective**, and in particular as rational **degrees of belief** in states of affairs. And the classic use case for Bayesian reasoning is in **updating** one's subjective probability about something, **conditional on** some new evidence that comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Famous Example\n",
    "\n",
    "Suppose some rare disease affects 1 in 100,000 people. There is a test for it, though it is imperfect: 5% of the people who have the disease will test negative and 4% of the people who don't have the disease will test positive for it. You take the test and test positive. Before the test the probability that you had the disease was only 1 in 100,000. But now, with this new information of the positive test, how should you judge the probability that you have the disease?\n",
    "\n",
    "We can use **Bayes's Theorem**:\n",
    "\n",
    "$\\large P(h | e) = \\frac{P(e | h)P(h)}{P(e)}$.\n",
    "\n",
    "Terminology:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>term</th>\n",
    "        <th>name</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>P(h)</td>\n",
    "        <td>prior</td>\n",
    "    <tr>\n",
    "        <td>P(h|e)</td>\n",
    "        <td>posterior</td>\n",
    "    <tr>\n",
    "        <td>P(e|h)</td>\n",
    "        <td>likelihood</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>P(e)</td>\n",
    "        <td>scaler</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "That is, for our problem: The (new) probability that I have the disease (i.e. the posterior) is equal to the unconditional probability that I have the disease (i.e. the prior) multiplied by the probability that someone who has the disease tests positive (i.e. the likelihood) divided by the probability of testing positive (i.e. the scaler).\n",
    "\n",
    "To calculate the denominator, we'll need to make use of the **Law of Total Probability**.\n",
    "\n",
    "Remember that the Law of Total Probability tells us how to calculate an unconditional probability given a partitioning collection of conditional probabilities. In a Bayesian context, we can think of this law as saying: Suppose there are $n$-many hypotheses $h_1, ... , h_n$, that could explain some bit of evidence $e$. If I know all of the **conditional** probabilities $P(e | h_1), ... , P(e | h_n)$, then I can use the Law of Total Probability to calculate the **unconditional probability** $P(e)$.\n",
    "\n",
    "In our case, there are only two possible hypotheses: Either I have the disease or I do not.\n",
    "\n",
    "So if we put the Law of Total Probability together with Bayes's Theorem here we get:\n",
    "\n",
    "$\\huge P(h_1|e) = \\frac{P(h_1)*P(e|h_1)}{P(h_1)*P(e|h_1)+P(h_2)*P(e|h_2)}$, where:\n",
    "\n",
    "$h_1$: I have the disease\n",
    "\n",
    "$h_2$: I do not have the disease\n",
    "\n",
    "$e$: I test positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.00001 * 0.95) / (0.00001 * 0.95 + 0.99999 * 0.04)\n",
    "\n",
    "# Notice that the likelihoods (P(e | h_1) and P(e | h_2)) do NOT sum to 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that I have the disease is still less than 1 in 4000!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Monty Hall Problem\n",
    "\n",
    "Bayesian reasoning applies naturally and neatly to the famous Monty Hall Problem.\n",
    "\n",
    "The situation is this: Monty Hall, classic and original host of the TV game show \"Let's Make a Deal\", has told you, the contestant, that there is some fantastic prize behind one of three curtains. Suppose, without loss of generality, that you select Door \\#1.\n",
    "\n",
    "Now here's where the story gets interesting. Hall then shows you that the prize is NOT behind one of the other two curtains. Let's suppose, again without loss of generality he opens Door \\#2.\n",
    "\n",
    "Note a few things at this point:\n",
    "\n",
    "1. Hall will never show you what's behind the door you selected.\n",
    "2. Hall will never show you where the good prize is.\n",
    "3. \\#1 and \\#2 notwithstanding, Hall will always be able to show you what's behind one of the doors without revealing where the prize is.\n",
    "\n",
    "Then Hall asks you if you'd like to *switch* your pick to the remaining door.\n",
    "\n",
    "The question is: Do you have any reason to switch? Or do you have any reason to stick with your original choice?\n",
    "\n",
    "It is natural to reason in the following way: Either the prize is behind Door \\#1, the one I picked, or it is behind Door \\#3. Clearly, the fact that Hall opened Door \\#2 could not have had any effect on the location of the prize, so there is no reason to prefer Door \\#1 or Door \\#3. So the probability that the prize is behind either of the two remaining doors is 50%.\n",
    "\n",
    "But this reasoning is wrong! The fallciousness of this reasoning was pointed out in a magazine column in the 1980s, although the writer of the column received much criticism from many \"experts\" who were arguing that the reasoning was perfectly sound. (There's a nice article about the history here: https://priceonomics.com/the-time-everyone-corrected-the-worlds-smartest/.)\n",
    "\n",
    "The truth is that you ought to switch! The truth is that, while there is only 1 chance in 3 that the door you originally selected has the prize, there are 2 chances in 3 that the other door has the prize.\n",
    "\n",
    "Let's see if we can proceed through this puzzle in Bayesian terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior Probability that the Prize Is Behind Your Door\n",
    "Let's calculate the posterior probability that the prize is behind Door \\#1, given the evidence that Hall shows you that it is not behind Door \\#2:\n",
    "\n",
    "What is the *prior* probability that the prize is behind Door \\#1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_1 = 1/3\n",
    "prior_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider the likelihood of the evidence: What is the probability that Hall would show you that it is not behind Door \\#2, given the hypothesis that the prize is behind Door \\#1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "likelihood_1 = 0.5\n",
    "likelihood_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's calculate the probability of the evidence itself:\n",
    "\n",
    "There are three salient hypotheses to consider:\n",
    "\n",
    "- $h_1$: The prize is behind Door \\#1.\n",
    "- $h_2$: The prize is behind Door \\#2.\n",
    "- $h_3$: The prize is behind Door \\#3.\n",
    "\n",
    "And our evidence is:\n",
    "\n",
    "- $e$: Hall shows you that the prize is not behind Door \\#2.\n",
    "\n",
    "We can calculate the probability as follows:\n",
    "\n",
    "$P(e) = P(e | h_1)\\times P(h_1) + P(e | h_2)\\times P(h_2) + P(e | h_3)\\times P(h_3)$.\n",
    "\n",
    "We've already made the calculation for the first term:\n",
    "\n",
    "$P(e | h_1)\\times P(h_1) = \\frac{1}{2}\\times\\frac{1}{3} = \\frac{1}{6}$.\n",
    "\n",
    "Now:\n",
    "\n",
    "The probability that Hall would show you Door \\#2, given that the prize is behind Door \\#2, is *zero*. (This is by the rules of the game.)\n",
    "\n",
    "The probability that Hall would show you Door \\#2, given that the prize is behind Door \\#3, is *one*. (Here Hall's hand would be forced, since he can, by the rules, show you neither the door that you chose (\\#1) nor the door with the prize (\\#3).)\n",
    "\n",
    "So we have:\n",
    "\n",
    "$P(e) = \\frac{1}{2}\\times\\frac{1}{3} + (0)\\times\\frac{1}{3} + (1)\\times\\frac{1}{3} = \\frac{1}{2}$.\n",
    "\n",
    "We are finally in a position to calculate the posterior! We have:\n",
    "\n",
    "$\\large P(h_1 | e) = \\frac{P(e | h_1)\\times P(h)}{P(e)} = \\frac{1 / 6}{1 / 2} = \\frac{1}{3}$.\n",
    "\n",
    "That is, our updated probability that the prize is behind Door \\#1 is just the same as the prior. It's still just $\\frac{1}{3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior Probability that the Prize Is Behind the Other Door\n",
    "\n",
    "The posterior probability that the prize is behind Door \\#3, given the evidence that Hall has shown you what's behind Door \\#2, ought to work out to $\\frac{2}{3}$. Let's verify this:\n",
    "\n",
    "What is the *prior* probability that the prize is behind Door \\#3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_3 = 1/3\n",
    "prior_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the likelihood: What's the probability that Hall would show you Door \\#2, given that the prize is behind Door \\#3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_3 = 1\n",
    "likelihood_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already made our calculation of the evidence. From before we had:\n",
    "\n",
    "$P(e) = \\frac{1}{2}\\times\\frac{1}{3} + (0)\\times\\frac{1}{3} + (1)\\times\\frac{1}{3} = \\frac{1}{2}$.\n",
    "\n",
    "So now we are in a position to calculate the posterior probability that the prize is behind Door \\#3. We have:\n",
    "\n",
    "$\\large P(h_3 | e) = \\frac{P(e | h_3)\\times P(h)}{P(e)} = \\frac{1 / 3}{1 / 2} = \\frac{2}{3}$.\n",
    "\n",
    "Given that Hall shows us what's behind Door \\#2, we should now update our degree of belief that the prize is behind Door \\#3 to $\\frac{2}{3}$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use `monty_hall.py`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh.play_mh(my_sel=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh.stats_mh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swapping = []\n",
    "not_swapping = []\n",
    "for _ in range(100000):\n",
    "    swapping.append(mh.stats_mh(swap=True))\n",
    "    not_swapping.append(mh.stats_mh(swap=False))\n",
    "print(sum(swapping) / 100000)\n",
    "print(sum(not_swapping) / 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihoods and Estimation\n",
    "\n",
    "The idea of using evidence (including experimental results) to guess at parameter values or at underlying distributions is very powerful, but it is an idea that doesn't make any sense under traditional (objectivist) understandings of probability. Once we move, however, to the Bayesian framework, under which probabilities are to be thought of as subjective (but rational!) degrees of belief, we can make sense of:\n",
    "\n",
    "- assigning probabilities to a single event\n",
    "- assigning probabilities to various distributions as underlying the data we've collected\n",
    "- MLE and MAP techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation\n",
    "\n",
    "Harry the Hat, a famous magician, gives you a coin as a gift. It looks like an ordinary penny, but you know that Harry has some trick coins in his collection, so you're curious to know whether it's fair. So you flip it five times. You get three heads and two tails. And so now, with these results, you're musing about the possible weightings of the coin.\n",
    "\n",
    "Notice that this is the reverse of a more stereotypical situation where you *know* the weighting of the coin and then calculate the probabilities of various coin-flip results. In fact this sort of problem makes sense only in a *Bayesian* context where we're allowing ourselves to assign probabilities to states of the world (the weighting of the coin, in the present case).\n",
    "\n",
    "But given this allowance, we can think about the associated ***likelihood*** as a function of the coin's weighting: $P(e | h) = P(3H, 2T | p)$, where $p$ is the probability of getting a H on one toss.\n",
    "\n",
    "A natural move to make, then, is to find where this function achieves a ***maximum*** value. And, given our understanding of combinatorics and of the Binomial Distribution, we know how to calculate the values of this function.\n",
    "\n",
    "The probability of getting three heads and two tails on five tosses, with $p$ defined as above, is:\n",
    "\n",
    "$\\binom{5}{3}\\times p^3\\times(1 - p)^2$.\n",
    "\n",
    "So let's graph this function for several values of $p$ and find where it hits its maximum value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0, 1, 100)\n",
    "Y = 10 * X**3 * (1-X)**2\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(X, Y)\n",
    "plt.title('Maximum Likelihood')\n",
    "plt.xlabel('p')\n",
    "plt.ylabel(f'P(3H, 2T | p)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we notice about this value of $p$ where the likelihood finds its maximum? How is it related to our sample results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculus Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we don't have to rely on eyeballing a graph to calculate the maximum of a function. We can use an old calculus trick:\n",
    "\n",
    "Consider the maximum of a function $f$; suppose it occurs at $x = x_{0}$. Now $f'$, the derivative of $f$, tells us how that function is changing at any value of $x$. And so that means that:\n",
    "- $f'(x) > 0$ for $x < x_{0}$; and\n",
    "- $f'(x) < 0$ for $x > x_{0}$.\n",
    "\n",
    "And in particular we can show that $f'(x_{0}) = 0$. Similar reasoning shows the same fact about minima.\n",
    "\n",
    "Thus if we have a differentiable function for which we seek its maxima or minima, we just need to solve the equation $f'(x) =  0$ for $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum A Posteriori Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum likelihood may be a good estimate of the fairness of Harry the Hat's coin, but recall that we have *no prior beliefs* about the coin's weighting.\n",
    "\n",
    "If we *do* have some prior belief about the fairness of the coin, then we can use Bayes's Rule to update our belief, and this is what ***maximum a posteriori (MAP)*** estimation is all about.\n",
    "\n",
    "Often the prior and the likelihood functions work together in such a way that the posterior will have exactly the same distribution as the prior. This is called [**conjugacy**](https://en.wikipedia.org/wiki/Conjugate_prior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: The Worst Hitter in MLB History\n",
    "\n",
    "In this project FIS's own Greg Damico uses a MAP technique to explore the question of who could fairly be called the worst hitter in the history of the major leagues.\n",
    "\n",
    "https://github.com/gadamico/Sports-Data/\n",
    "\n",
    "\n",
    "https://www.linkedin.com/pulse/worst-hitter-history-major-league-baseball-bayesian-approach-damico/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up: Analytically Intractable Problems and Indeterministic Approximation Methods\n",
    "\n",
    "In general, there will be infinitely many possible probability distributions responsible for the observations we've made.\n",
    "\n",
    "A Bayesian might make use of *Monte Carlo sampling* here:\n",
    "\n",
    "(i) choose some distribution-defining parameters randomly; <br/>\n",
    "(ii) calculate the likelihood of the data having come from that distribution; <br/>\n",
    "(iii) \"step\" randomly to a new set of parameter values and do (i) and (ii) again <br/>\n",
    "(iv) if the newly calculated likelihood is higher than the old, then take another \"step\" *from* those new parameter values <br/>\n",
    "(v) repeat until your likelihood isn't getting any higher.\n",
    "\n",
    "Monte Carlo sampling is often used to approximate values of definite integrals that have no analytic solution. The classic example here is [the estimation of pi](https://en.wikipedia.org/wiki/Monte_Carlo_integration#Example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use `piApprox.py`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_approx(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses = []\n",
    "for _ in range(100):\n",
    "    guesses.append(pi_approx(100))\n",
    "np.mean(guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses = []\n",
    "for i in range(1000):\n",
    "    guesses.append(pi_approx(1000))\n",
    "np.mean(guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up: Objective and Subjective Probability\n",
    "\n",
    "Philosophers wonder about the best way to understand what probabilities are. The main division is between those who want to understand probabilities _objectively_ and those who want to understand probabilities _subjectively_.\n",
    "\n",
    "### Historical Relevance\n",
    "In the early twentieth century, the quantum theory being developed by physicists was saying that the location (etc.) of a particle could be represented by a probabilistic wave function that gave probabilities for the particle to be in one place rather than another. And this question of how to understand these probabilities reared its head. Albert Einstein argued that they could only be interpreted subjectively, but the dominant interpretation today is that there is a kind of indeterminacy in the universe itself.\n",
    "\n",
    "### Objective Probability\n",
    "The paradigmatic theory of _objective_ probability is frequentism, which says that probabilities are a measure of the long-run behvior of physical systems. To say that a die has a 1/6 chance of coming up \"6\" when tossed, for example, is to say that, in the long run as the number of tosses increases without bound, the number of \"6\"s rolled will constitute one sixth of all tosses.\n",
    "\n",
    "On this point of view, **we cannot speak meaningfully of the probability of a single event**. Once a die has been rolled, there is no non-trivial probability of its having come up \"6\" or not. Either it did (in which case the probability is 1) or it did not (in which case the probability is 0).\n",
    "\n",
    "Similarly, **we cannot speak meaningfully of the probability of a parameter having a certain value, or of a hypothesis being true**. The frequentist will reject the idea of a (meaningful) probability of a die being unfairly weighted. Either it is or it is not.\n",
    "\n",
    "\n",
    "### Subjective Probability\n",
    "The paradigmatic theory of _subjective_ probability is Bayesianism, which says that probabilities are better understood as rational _degrees of belief_. The standard of rationality is necessary here to assure that these degrees of belief will conform to the probability calculus.\n",
    "\n",
    "If probabilities are degrees of belief, then it _does_ make sense to apply them to parameters or to hypotheses. The probability of a die being unfairly weighted would simply represent what it would be rational to believe about the die with respect to its being weighted or not.\n",
    "\n",
    "Now: Crucially, what it is rational to believe about the die with respect to its being weighted or not _is a function of what we know about the die!_\n",
    "\n",
    "In particular, if we gain the evidence (or knowledge) that the die has been rolled 100 times and come up \"5\" 90 times, then this would have (or, rather, *ought, rationally, to have*) a significant impact on our degree of belief with respect to the weightedness of the die. This is the sort of idea that Thomas Bayes had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
